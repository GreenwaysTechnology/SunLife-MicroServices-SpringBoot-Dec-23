				 Microservices
.....................................................................................

Application(software system) Develpment patterns:
.................................................

Network based Applications - Distributed Application

Application has layers:

1.User interface layer
2.Application biz layer
3.Data Layer /Repository layer

Histrory of Architecture of Distributed Application

Distributed means, application is broken into multiple parts and each part put into multiple hosts/machines, connect those via network.

1.Mainframework based distributed
   1.Application biz layer
   2.Data Layer /Repository layer
 Where as User interface layer is kept in dump terminals connected to mainframworks.

Drawbacks:
1.Too costly
2.scalability is too difficult.


Advantage:
1.High security
2.Centeralized management.

2.Client Server Architecture

2.1.Main framework based client -server , where as mainframe acts as server and digital computers act as clients.
 
2.2.Digital computer based client - server architechture
   Servers and clients are digital computers

Based on this we can classify the applications  layered /tiered concept


1.single tier/layer
   client,server,database every is kept in one single machine...
2.two tier/layer
  user interface is kept in client machine,
  data logic and biz logic is kept in server machine
 both machines are connected via networks

          "This arch based on LAN /WAN"

3.three tier /layer

 This arch based on "internet network" and web computing

   client - client machine
   server - biz logic is kept inside another machine
   database - is kept inside another machine

 Client is browser
 Server BIZ logic is kept as "web Applications"
 Database is accessed by "Server side technologies - J2EE,ASP/.net,PHP,....

4.N-tier / layer

 Client is browser
 Server BIZ logic is kept as "web Applications"
   -Again spilt into multi layered
 Database is accessed by "Server side technologies - J2EE,ASP/.net,PHP,....
 In 2000 , J2EE introduced n-tier client server  

  browser -------web application(servlets/jsp)----EJB----Messaging/Databases(JMS/JDBC/Middlewares)

 Spring based N-tier client server arch:

  browser -------web application(spring mvc)---Spring serivices----Spring data----Messaging/Databases(JMS/JDBC/Middlewares)
.....................................................................................
		    How to build N-tier distributed Applications
.....................................................................................

Steps/Process:

1.Domain Modeling

  Banking, Online Food Delivery App, Ecommerce Domain

2.Select technology

   if your app is based on web and internet.
  Steps:
   1. Database -  Oracle 
   2. MOM - RabbitMQ,IBM MQ,Microsoft MQ
   3. Development Technology
       Java/JEE  - Why you go with specific implmentation technologies?
       .Net
       Php

3.Development and release methodology
   Waterfall -  traditional dev , release


Any domain consist of various modules
 -Accounts
 -Loans
 -Customers
 -Card
etc.....
 
4.Testing
   Once the development is over, the app is going to be under testing

5.Production
   Once the app is tested fully, ready for production.

6.Maintance
   Once the app in the production, it goes on maintaince...

if any app is built based on the above methodology, which is called as "Monolithic"
.....................................................................................
 Challanges in the application development,testing,relase,Production,maintaince
....................................................................................


1.Every thing has to go step by step -  this increase cost , time waste,resource waste

Companies like Amazon,Netflix who wanted fast development,test,release,maintaince : Dynamic  methodology to build applications -  No downtime,
One module takes more time ,another module takes less time, because of one module , other module should not wait.

2.Technology bottleneck - Mono technology
 
 The whole application is built using single technology - Java - vendor lock
 The whole application targets single database - Oracle /Mysql/Microsoft SQL server..

3.Employing security layer is more complicated

4.Deployment / Production.

   The dev and prod env is completly different
   Bare deployment models
   VM based deployment...

...................................................................................
			  New way of building apps


1.Automatation is key concept 

   to anays,dev,test,release,prod,maintaince

Agile :(Requirement Analysis)

  Agile is an iterative approach to project management and software development that helps teams deliver value to their customers faster and with fewer headaches. Instead of betting everything on a "big bang" launch, an agile team delivers work in small, but consumable, increments

Breaks the application into smaller and smaller.
 - fast delivery with quality on time.


Requirments are highly dynamic, cant be freezed,since it is dynamic start development,test,release,deploy peridically.


We need automation, through which  automatically only we can achive fast delivery -
 in order to automate, a new technology was created "Dev Ops" - Dev + Operations togther.

Distributed source code repo - git

Pipe lines tools  - 
   Jenkins -(Continuous Integration)

Requirement---> Dev---push the code to source code repo---|CI Tool---Compile--Build/pack--Testing-Deployment(CD)

Every thing is here Continuous happens

Continuous Req Analysis
Continuous Dev 
Continuous release /build
Continuous test
Continuous release /build
Continuous deployment
Continuous tracing and monitoring

This process applied on every module in the applications
OrderManagement 
   Continuous Req Analysis,Dev,release,test,deployment,tracing,monitoring

CustomerManagement 
   Continuous Req Analysis,Dev,release,test,deployment,tracing,monitoring


if any app is built based on the above methodology, that application is called as 				  "MicroService"

...................................................................................
	     How to convert existing monolithic apps into microservices
.....................................................................................

Scalling means expanding either software or hardware resources..
if you scale software- horiziontal
if you scale hardware -vertical 

Why Scale Cube?

Increase performance
Make you app highly available..

X,Z => Scale instance of your app

X- based on Built in routing algorthims
Z- Custom routing algorthims

Assume that your app is already running in production,based on Monolithic model,
You have applied X scalling, that means your monolith app is running multiple instances.

Next step, you got assigment, you have to convert existing application(monolithic) into microservices?

How to begin?
 Apply scale cube pattern........ Y scalling 


Y-Axis scalling talks about how to spilt the existing monolith  application into micro services based on "functionals aspects" - Service


A Service is a "mini application" that implmements narrowly focused functionality.
Such as orderManagement,customer Management, and so on....

Some services are scalled based on "X-axis" and some services are "Z-scalling".


Your App
   -Y scalling
       -X or Z scalling....

The high level definition of microservices architecture(microservices) is an architectural style that "functionally decomposes an application into set of services(mini applcation)

In Monlolith app the app is broken into "modules" where as microservice break as services(mini application)..


What Microservices offers?

1.Microservices offers "form of modularity"
2.Each service has its own database - Customer Service may use "Mongodb'
  where as payment service may use "Oracle database"

Benefits of the microservice architecture

 It enables the continuous delivery and deployment of large, complex applications.
 Services are small and easily maintained.
 Services are independently deployable.
 Services are independently scalable.
 The microservice architecture enables teams to be autonomous.
 It allows easy experimenting and adoption of new technologies.
 It has better fault isolation
....................................................................................
			How to design and implment microservices

The microservices is all about practices followed,implemented, and tested in real time production grade applications in various companies like amazon,netflix,google,microsoft.

The many community people joined  togther who formed the pattern language in order to begin development of Microservices - Microservice pattern language, design patterns
.....................................................................................
	      Decision Pointers when start building app


Step : 0 - May be for new Application(new Requirement) or existing Application
			
Requriment for building online food delivery:
	
1.You are developing a server-side enterprise application.

2.It must support a variety of different clients including desktop browsers, mobile browsers and native mobile applications. 

3.The application might also expose an API for 3rd parties to consume.

4.It might also integrate with other applications via either web services or a message broker.

5.The application handles requests (HTTP requests and messages) by executing business logic; accessing a database; exchanging messages with other systems; and returning a HTML/JSON/XML response. 

6.There are logical components corresponding to different functional areas of the application.	
....................................................................................
			How to design and implment microservices

The microservices is all about practices followed,implemented, and tested in real time production grade applications in various companies like amazon,netflix,google,microsoft.

The many community people joined togther who formed the pattern language in order to begin development of Microservices - Microservice pattern language.
.....................................................................................
			  Pattern Languages
.....................................................................................

Pattern is a resuable soultion to  a problem that occurs in a particular context.

Christopher Alexander writings inspired the software community to adopt the concept of patterns and patterns language, The book Design patterns: Elements of Resuable Object oriented Sofware - GOF patterns.


Elements of patterns.

Every Pattern has sections

1.Forces
2.Result Context
3.Related patterns

Forces: The issues that you must address when  sovling a problem.

 The forces section of a pattern describes the forces(issues) that you must address when solving a problem in a given context.

Sometimes forces can conflict, so it might not be possible to solve all of them.

Which issues(forces) are more important dependens on the context.

eg:

When you write code in a reactive style , has better performance than non reactive sync code.
But it more difficult to understand.

Resulting Context:
..................
 The force section of a pattern describes issues(forces) that must address when a solving a problem in a given context.

The result context section of a pattern describes the consequences(advantages and disadvantages) of applying the pattern.

It consistts of three parts

1.Befnifits: 
   The benefits of the pattern, including the forces that have been resolved.
2.Drawbacks:
   The drawbacks of the pattern, including, un resolved forces.
3.Issues
    The new Prolmes that have been introduced by applying the pattern.

The result ing context provides a more complete and less biased view of the solution
which enables better decisions.

Related Patterns:
 The related patterns describe the relationship between the pattern and other patterns.


There are five types of relationship between patterns.

Predecessor – a predecessor pattern is a pattern that motivates the need for this pattern. For example, the Microservice Architecture pattern is the predecessor to the rest of the patterns in the pattern language except the monolithic architecture pattern

if i have selected microservice, then only i can think about other patterns of microserivce else i cant.

Successor – a pattern that solves an issue that is introduced by this pattern. 
For example, if you apply the Microservice Architecture pattern you must then apply numerous successor patterns including service discovery patterns and the Circuit Breaker pattern.

Alternative – a pattern that provides an alternative solution to this pattern. For example, the Monolithic Architecture pattern and the Microservice Architecture pattern are alternative ways of architecting an application. You pick one or the other.

Generalization: - A Pattern that is a general soultion to a problem for eg if you want to host a service , we have different implementations like single serivce per host pattern, single service on multiple hosting etc...

Specialiation: - A specialized form of  a particular pattern -  For eg deploy a service as container pattern is spacilzation of a single service per host.
.....................................................................................
....................................................................................
		   Microservice arichitecture pattern language
...................................................................................  
The Microservice pattern language is a collection of patterns that help you architect an application using the microservice architectures.

Infrastructure Patterns:
  Thses solves problems that are mostly infrastructure issues outside of development.

Application patterns:
  These are for related to development

Application Infrastructure:
   Application related infrastructures like containers
.....................................................................................
....................................................................................
		 Patterns for Decomposing an Application into services	


1.Decompose by business capability 
	 |
	 |
2.Decompose by subdomain

3.SelfContained Service

4.Service Per Team
.....................................................................................
			    Design patterns in Microservices
....................................................................................

Application Architecture Pattern
   For building n-tier client server distributed application.

-Monolithic architecture
-Microservice architecture

Decomposition Pattern -Y scalling

1.Decompose by business capability 	 
2.Decompose by subdomain
3.SelfContained Service
4.Service Per Team


MicroService Architecture Pattern------>Depcompostion Pattern

Decompose by business capability 

If you are going to build online store.

Business capability:
Product Catalog Management
Inventory Management
Order Management
Delivery Management.

Alternate Pattern

Decompose by SubDomain:
 Decompose the problem based on DDD principles.

....................................................................................
				Data Management
....................................................................................

Core Pattern:
1.Database Per Service Pattern
2.Shared Database

Note:
  if you take any data related patterns, "Transactions" are very important.


1.Database Per Service Pattern leads/succeeds other patterns
  Domain Event
  Event Sourcing 
  Saga - Transaction
  CQRS
  API Composition
 
..................................................................................
	    Advance Data Management Pattern -Transactional Messaging Pattern   
..................................................................................

1.Transactional outbox
    2.Transactional log tailing
     or
    3.Polling publisher

   2.1.Idemponent Consumer
.....................................................................................
			Communication Style Patterns
...................................................................................

Service = Mini Application

MiniApplication = Collection of Programs

Collection of programs in Java = Collections of classes

Collections of classes = Collections Objects.

Object/Class = Collection of  state and behaviour

State = data
Behaviour=methods

Object = methods

methods = API

API will DO three things

1.write - update,remove,insert
2.read
3.process

class OrderService {

   @Autowrited
   private OrderRepository orderRepo;
   //API
   public List<Order> findAll(){
	orderRepo.findAll()
   }
 
}

Types of API:
1.local api
   api which are called with in same runtime by other apis
2.remote api
  api which are called outside runtime via networks 

How to build remote api?

 Based on protocals

1.HTTP Protocal.

 if you design your api based on HTTP protcal, those apis are called as "WebServices"

Web Service:
  RESTFull WebServices,SOAP WebServices

REST API = Program

In java => classes

In Web Services classes are called "End Points"

In Micro services -Services can be  represented as "WebServices"

Rest WebService------>http-----RestWebservice => HTTP based Microservice

Rest WebService------>http-----Graphql => HTTP based Microservice

Rest WebService------>http/2 over tcp------GRpc Service=> TCP based Microservice

Rest Web Service ---->TCP/MOM-------------->Messaging Service -Middlewares

Communication Sytle patterns:

1.RPI patterns
   REST,gRPC,Apache Thrift - RPI implementations
2.Messaging
   Any Messaging middlewares - RabbitMQ,IBM MQ,MicroSoft MQ - MQTT,AMQP
   Streaming platforms - Apache Kafka,Confluent Kafka
   2.1.Idemponent Consumer
3.Domain Specfic Protocal
   SMTP - Mail Service   

.....................................................................................
			   Deployment Patterns
.....................................................................................

Once the services(applications) are ready, we can move the application into production.

Production Related Patterns:

Deployment Environment/Plattforms

1.Bare Metal 
    Where as physical hardware, and operating system, Where we can provision our application.
    If you deploy java application.

  OS: Linux
  JRE- 17
  WebContainer -Tomcat
  Databases -MySql
  Streaming Platforms-Kafka
   
2.Virutal Machine 
   Oracle Virtual box 
    on VM , you can install os-linux
  JRE- 17
   WebContainer -Tomcat
   Databases -MySql
   Streaming Platforms-Kafka

3.Containerized Deployment
    It is lightweight vm - Docker and Kubernets
   JRE- 17
   WebContainer -Tomcat
   Databases -MySql
   Streaming Platforms-Kafka
  
4.Cloud 
   ->VM /container/bare 
  you can just deploy your app only,
  cloud may give you all softwares for you...


  "Cloud with containers are most preferable deployment for microservices"

Design patterns:

Bare Metal:
 1.Multiple services instances per host
 2.Service instance per host
VM
 1.Service instance Per VM
Container
 1.Service instance per Container
Cloud
 1.server less deployment
 2.Service deployment platform
 3.container and cloud

			      
		if your app deployement is in container 
				  or 
    			       in cloud
				  or 
			   Container with cloud
				 or 
			   in any Virtualized Env

if any micro service(application) is running in containerized env like kubernets(docker).

Challanges:
 1.suppose the application is accessed by other application or external application
   we need to communicate the application with help of "host:port".
 if application is running Virtualized env, "host and port" is not static,it would be dynamic.

 if it is dynamic then how other microservices, and external application, how they can communicate.
			
To solve the problem of identifying the services which are running in Virtualized env
 
				Advanced Communication Patterns
				(Service Registry and Discovery)


When we apply this pattern, services never communicate "directly", because they dont know each other due to "dynamic location",so they use broker to communicate, broker will have all service information-Service Registry

Service Registry Patterns:

1.Client side service Discovery
2.Server-side service Discovery
   ->Service Registry
  	->Self Registration
   	->ThridParty Registration

.....................................................................................
                     Services are running in Virtualized Env
		     Services are talking via Service Registry
		What if i any service is down / Slow / Throwing Exception
					
Microservices provides a Design patterns to handle failures and slow calls

			 Service Reliablity Patterns

1.Timeout Pattern
2.Bulk Head Pattern
3.Retry Pattern
4.Circuit Breaker Patterns
.....................................................................................
			    Configuration Data and Its patterns

Every application which requires configuration data,the configuration data may be connection strings,api tokens,application settings etc...

In Java application, configuration data is kept inside properties or yml files...

What if in micro serivces, the configuration is need to be shared across the application?

We have design pattern to centeralize configuration data/information.

1.Microservice Chassis
2.Service Templates
3.Externalized COnfiguration
.....................................................................................
		   Micro services are ready in production
			Now we need to expose to
		 other Applications- User interface applications

Microservices provide you a design pattern, called

External API patterns:

1.API Gate ways
  2.Back End for FrontEnd

.....................................................................................
		     Micro services are ready in production
		We have exposed our microservices via API Gateways
			  How to secure them?

Security Patterns

1.Access Tokens
   -Authentecation
   -Authrozation
   -SSL
   -Policies
....................................................................................
			Now your Micro service is in Production
				Next what should i do
			     Your App in Maintaince 

Monitor Your apps......

Observablity Design patterns:

1.Log Management/Aggregation Pattern
2.Application Metrics Pattern
3.Audit Logging pattern
4.Distributed Tracing
5.Exception Tracker Pattern
6.Health Check API pattern
.....................................................................................
			How to apply/select pattern
.....................................................................................

Pattern Elements

1.Context
2.Problem
3.Forces
4.Solution
5.Resulting Context
6.Related Patterns
7.Anti patterns
8.Implementation using program - Spring.
.....................................................................................
			Microservices Implementations
....................................................................................

Microservice is architecture that proposes many design patterns and principles, it is language and platform independant.

Java Microservices:
..................
 Java technology provides various microservices pattern implementations.


Spring boot
   Spring configuration system.
 Spring app can be configured
  1.XML - Legacy way of configuration.
  2.Java Config
   2.1.Manual Java Config
   2.2.Auto Java Config
        -Spring Boot

1.Spring Cloud
   It is  a project(module) brought into spring framework echo system

2.Quarkus

3.Eclplise Vertx

4.AKKA with Microservices/Play

5.Micronaut
....................................................................................
			Spring Cloud and implementations
...................................................................................

if you are new spring echo system(old spring,spring boot),First you need to learn spring(core,web,data).


Micro service Application arch in Spring:
..........................................


	   Spring cloud Config  Spring Cloud CircuitBreaker Spring Cloud Discovery ..

		|                     |                       |

	         (Microservice Pattern language implementations)  			    
			       Spring Cloud		
				    |		  		 	                       .................................................
		  Spring Core,Spring Web,Spring Data - API Development
				   |
				Spring Boot
			           |
			     Spring Framework

.....................................................................................

Steps:

1.Understanding REST API(Could be any api-graphql,grpc,MoM) development ,with data sources(mysql,postresql,nosql databases)


2.Pick up design pattern.
.....................................................................................
				  Data 
			Event Sourcing and Domain Events
			   Event Driven Microservices
....................................................................................

An Event-microservices architecture is an approach to software development where decoupled microservices are designed to communicate with one another when events occur.

Event sourcing
Domain event - Inspired From Domain Driven Design
 
  Both are same , which are different from only based model we select.
if you select DDD, you can follow designing "events" using domain events.


1.Context
2.Problem
3.Forces
4.Solution
5.Resulting Context
6.Related Patterns

1.Context
  A service "command" typically needs to create/update/delete aggregates in the database and send messages/events to a message broker.

Note: 
command-verb-method
aggregates - A graph of objects that can be treated as a unit. (From DDD)

 "Event Sourcing is an alternative way to persist data". In contrast with "state-oriented" persistence that only keeps the latest version of the entity state, Event sourcing stores each state mutation as separate record called event.

When user starts interaction , when making order...
   
 Order :                 Order :                Order
  Number : 1220            Number : 1220         Number : 1220
  status : STARTED  ---->  status :PENDING ----> status: CONFIRMED | REJECTED
  total : 200               total : 200          total : 200
  paid : 0                  paid: 0              paid : 5000
   
  
UPDATE QUERY - status=STARTED
UPDATE QUERY - status=PENDING
UPDATE QUERY - status=CONFIRMED

History of Transcation

started    pending     confirmed
 |
------------------------------------------------------------------
 |          |              |
   
log      log              log  ------>EVENT store -can be any db or brokers
			  

Problem
 How to atomically update the database and send messages to a message broker?


Solution
   A good solution to this problem is to use event sourcing. Event sourcing persists the state of a business entity such an Order or a Customer as a sequence of state-changing events.

Resulting context

1.It solves one of the key problems in implementing an event-driven architecture and makes it possible to reliably publish events whenever state changes.

2.Because it persists events rather than domain objects, it mostly avoids the object‑relational impedance mismatch problem.

3.It provides a 100% reliable audit log of the changes made to a business entity
 It makes it possible to implement temporal queries that determine the state of an  entity at any point in time.

4.Event sourcing-based business logic consists of loosely coupled business entities that exchange events. This makes it a lot easier to migrate from a monolithic application to a microservice architecture

Related patterns
..................
1.The Saga and Domain event patterns create the need for this pattern.
2.The CQRS must often be used with event sourcing.
3.Event sourcing implements the Audit logging pattern.



Eventsourcing with "eventStore as Database table"
.................................................


Implementation:

Use Case:

Mr Subramanian has a shop
He is sells electrnic items like mobile phones, laptops etc
He wants to keep track of the stock in his shop.


App functionality:

1.Add new stock
2.Remove existing stock
3.find current stock of particular item.

Initally this app built using traditional way : without event sourcing pattern.

There is a table stock table , when ever new product added stock is added or when ever product is removed(sold), stock is updated.

when ever stock is added or removed current state updated.

This same operation is done by another co worker of subramanian who is mr Ram.

One day Subramanian got doubt something went wrong in the stock, now he realized existing system cant track what happened.
 when ever new stock is added or removed existing one, we cant track it.

He found a soultion to sove this issue by "Event Sourcing Pattern"

You can capture user events and add them in "Event Store"

Modling Events:
"StockAddedEvent"
"StockRemovedEvent"
 
You can store these events in relational database or event platforms like kafka.

Steps:

2.create Spring boot project...

pom.xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<parent>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-starter-parent</artifactId>
		<version>3.2.0</version>
		<relativePath/> <!-- lookup parent from repository -->
	</parent>
	<groupId>com.sunlife</groupId>
	<artifactId>eventsourcing</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<name>eventsourcing</name>
	<description>Demo project for Spring Boot</description>
	<properties>
		<java.version>17</java.version>
	</properties>
	<dependencies>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-data-jpa</artifactId>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-web</artifactId>
		</dependency>

		<dependency>
			<groupId>com.h2database</groupId>
			<artifactId>h2</artifactId>
			<scope>runtime</scope>
		</dependency>
		<dependency>
			<groupId>org.projectlombok</groupId>
			<artifactId>lombok</artifactId>
			<optional>true</optional>
		</dependency>
		<dependency>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-test</artifactId>
			<scope>test</scope>
		</dependency>
	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
				<configuration>
					<excludes>
						<exclude>
							<groupId>org.projectlombok</groupId>
							<artifactId>lombok</artifactId>
						</exclude>
					</excludes>
				</configuration>
			</plugin>
		</plugins>
	</build>

</project>



application.yml
spring:
  datasource:
    url: jdbc:h2:mem:testdb
    driverClassName: org.h2.Driver
    username: sa
    password:

   jpa:
      database-platform: org.hibernate.dialect.H2Dialect

   h2:
     console:
       enabled: true
       path: /h2

....
Stock.java
package com.sunlife.eventsourcing;

import lombok.Data;

@Data
public class Stock {
    private String name;
    private int quantity;
    private String user;
}

Event: Record
package com.sunlife.eventsourcing;

public interface StockEvent {
}

package com.sunlife.eventsourcing;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class StockAddedEvent implements StockEvent {
    private  Stock stockDetails;
}

package com.sunlife.eventsourcing;

import lombok.Builder;
import lombok.Data;

@Builder
@Data
public class StockRemovedEvent implements StockEvent {
    private Stock stockDetails;
}
.....................
Repository:
 -Store Stock Information
 -Stock Event information

package com.sunlife.eventsourcing;

import jakarta.persistence.Entity;
import lombok.Data;

import java.time.LocalDateTime;

@Data
@Entity
public class EventStore {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private long eventId;
    private String eventType;
    private String entityId;
    private String eventData;
    private LocalDateTime eventTime;
}

package com.sunlife.eventsourcing;

import org.springframework.data.repository.CrudRepository;
import org.springframework.stereotype.Component;
import org.springframework.stereotype.Repository;

import java.time.LocalDateTime;

@Repository
public interface EventRepository extends CrudRepository<EventStore, Long> {

    Iterable<EventStore> findByEntityId(String entityId);

    Iterable<EventStore> findByEntityIdAndEventTimeLessThanEqual(String entityId, LocalDateTime date);
}

....
package com.sunlife.eventsourcing;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;

@Service
public class EventService {
      @Autowired
    private EventRepository repository;

    public void addEvent(StockAddedEvent event) throws JsonProcessingException {
        EventStore eventStore = new EventStore();
        eventStore.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventStore.setEventType("STOCK_ADDED");
        eventStore.setEntityId(event.getStockDetails().getName());
        eventStore.setEventTime(LocalDateTime.now());
        repository.save(eventStore);
    }

    public void addEvent(StockRemovedEvent event) throws JsonProcessingException {
        EventStore eventStore = new EventStore();
        eventStore.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventStore.setEventType("STOCK_REMOVED");
        eventStore.setEntityId(event.getStockDetails().getName());
        eventStore.setEventTime(LocalDateTime.now());
        repository.save(eventStore);
    }

    public Iterable<EventStore> fetchAllEvents(String name) {
        return repository.findByEntityId(name);
    }

    public Iterable<EventStore> fetchAllEventsTillDate(String name, LocalDateTime date) {
               return repository.findByEntityIdAndEventTimeLessThanEqual(name, date);

    }
}

Controller:

package com.sunlife.eventsourcing;


import com.fasterxml.jackson.core.JsonProcessingException;
import com.google.gson.Gson;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDate;
import java.time.LocalDateTime;

@RestController
public class StockController {
    @Autowired
    private EventService eventService;

    @PostMapping("/stock")
    public void addStock(@RequestBody Stock stockRequest) throws JsonProcessingException {
        StockAddedEvent event = StockAddedEvent.builder().stockDetails(stockRequest).build();
        eventService.addEvent(event);
    }

    @DeleteMapping("/stock")
    public void removeStock(@RequestBody Stock stock) throws JsonProcessingException {
        StockRemovedEvent event = StockRemovedEvent.builder().stockDetails(stock).build();
        eventService.addEvent(event);
    }

    @GetMapping("/stock")
    public Stock getStock(@RequestParam("name") String name) throws JsonProcessingException {
        Iterable<EventStore> events = eventService.fetchAllEvents(name);
        Stock currentStock = new Stock();
        currentStock.setName(name);
        currentStock.setUser("NA");
        for (EventStore event : events) {
            Stock stock = new Gson().fromJson(event.getEventData(), Stock.class);
            if (event.getEventType().equals("STOCK_ADDED")) {
                currentStock.setQuantity(currentStock.getQuantity() + stock.getQuantity());
            } else if (event.getEventType().equals("STOCK_REMOVED")) {
                currentStock.setQuantity(currentStock.getQuantity() - stock.getQuantity());
            }
        }
        return currentStock;
    }

    @GetMapping("/events")
    public Iterable<EventStore> getEvents(@RequestParam("name") String name) throws JsonProcessingException {
        Iterable<EventStore> events = eventService.fetchAllEvents(name);
        return events;
    }

    //History of events.
    @GetMapping("/stock/history")
    public Stock getStockUntilDate(@RequestParam("date") String date, @RequestParam("name") String name) throws JsonProcessingException {

        String[] dateArray = date.split("-");

        LocalDateTime dateTill = LocalDate.of(Integer.parseInt(dateArray[0]), Integer.parseInt(dateArray[1]), Integer.parseInt(dateArray[2])).atTime(23, 59);


        Iterable<EventStore> events = eventService.fetchAllEventsTillDate(name, dateTill);

        Stock currentStock = new Stock();

        currentStock.setName(name);
        currentStock.setUser("NA");

        for (EventStore event : events) {

            Stock stock = new Gson().fromJson(event.getEventData(), Stock.class);

            if (event.getEventType().equals("STOCK_ADDED")) {

                currentStock.setQuantity(currentStock.getQuantity() + stock.getQuantity());
            } else if (event.getEventType().equals("STOCK_REMOVED")) {

                currentStock.setQuantity(currentStock.getQuantity() - stock.getQuantity());
            }
        }

        return currentStock;

    }


}

How to test;

POST localhost:8080/stock

{
    "name":"IPhone",
    "quantity":10,
    "addedBy":"Ram"
}


GET localhost:8080/events?name=IPhone

[
    {
        "eventId": 4,
        "eventType": "STOCK_ADDED",
        "entityId": "IPhone",
        "eventData": "{\"name\":\"IPhone\",\"quantity\":34,\"user\":null}",
        "eventTime": "2023-12-13T17:19:32.961802"
    },
    {
        "eventId": 5,
        "eventType": "STOCK_ADDED",
        "entityId": "IPhone",
        "eventData": "{\"name\":\"IPhone\",\"quantity\":34,\"user\":null}",
        "eventTime": "2023-12-13T17:19:50.424197"
    },
    {
        "eventId": 6,
        "eventType": "STOCK_ADDED",
        "entityId": "IPhone",
        "eventData": "{\"name\":\"IPhone\",\"quantity\":10,\"user\":null}",
        "eventTime": "2023-12-13T17:21:26.872839"
    }
]

As of now how to store events with "Relational Database".
.....................................................................................
		  EventSourcing with External Events Store platforms
.....................................................................................

1.Kafka
2.eventStoreDb
3.CloudEventStore
4.Eventuate Tram

Kafka:
.....

What is Kafka?
  Apache Kafka is an open-source distributed event streaming platform.

What is Event?
   An Event is any type of action,incident,or change are "happening" or "just happened"
for eg:
  Now i am typing,Now i am teaching - happening
  Just i had coffee,Just i received mail, just i clicked a link, just i searched product - happened.

 "An Event is just remainder or notification of  your happenings or happened"

Events In the Softwares Systems:
................................
Every Software system has concept of "logs"

Log:
  Recording current informations.
 Logs are used in software to record activities of code.

...webserver initalize.... time.....
...webserver assigns port....
...webserver assigns host...

Logs are used to tracking,debuging,fixing errors etc..... 


Imgaine i need  somebody or somthing should record every activity of my life from the early moring when i get up and till bed.

  There is a system to record every events of your life that is called 
			      Kafka

	 Kafka is Event Processing Software , which stores and process events
...................................................................................
.....................................................................................
			Kafka Basic  Architecture
.....................................................................................

How kafka has been implemented?

   "Kafka is a software"
   "Kafka is a file(Commit log file) processing software
   "Kafka is written in java and scala" - Kafka is just java application
   "In order to run Kafka we need JVM"

How event is represented into kafka?

	Event is just a message.
        Every message has its own arch.
        In Kafka the Event/Message is called as "Record".
		Event(Record)

Event====>Record----------Kafka---will store into log file...
.....................................................................................
			 Sending Messages(Events) to Broker
.....................................................................................	
				Topics
....................................................................................

What is Topic?
  There are lot of events, we need to organize them in the system
  Apache Kafka's most fundamental unit of organization is the topic.

 Topic is just like table in the relational database.

  As we discussed already, kafka just stores events in the log files.

  We never write events into log file directly.

  As a developer we caputure events, write them into "topic" , kafka writes into log file from the topic.

  Topic is log of events, logs are easy to undestand

 Topic is just simple data structure with well known semantics, they are append only.

 When ever we write a message, it always goes on the end.

 When you read message, from the logs, by "Seeking offset in the log".

 Logs are fundamental durable things, Traditional Messaging systems have topics and queues which stores messages temporarily to buffer them between source and designation.

 Since topics are logs, which always permenant.

 You can delete directly log files not but not messages, but you purge messages.

 You can store logs as short as to as long as years or even you can retain messages indefintely.

Partition:
..........

 Breaking topic into multiple units called partitions.

Segments:
  Each partitions is broken up into multiple log files...
.....................................................................................
				 Kafka Broker
.....................................................................................

It is node or process which host kafka application, kafka app is java application.

if you run multiple kakfa process(jvms) on single host or mutliple host or inside vm or containers... : cluster.

Cluster means group of kafka process called broker.

Kafka has two software:
.......................

1.Data plane - Where actual records are stored - Brokers
2.Control Plane - which manages cluster- Cluster Manager

Control Plane:
1.Zookeeper - traditional control plan software.
2.KRaft- modern control plan software
.....................................................................................

Kafka Distribution:

1.Apache Kafka - core kafka -open source
2.Confluent Kafka - Confluent is company who is running by kafka creators, who built Enter prise kafka - community,enterprise...
.....................................................................................

How to work with kafka?

1.You kafka broker
2.You need application written in any language - can talk to kafka

Kafka provides cli tools to learn kafka core features, publishing,consuming etc....


How to setup kafka?

1.Desktop
   Linux,windows
2.Docker 
3.Cloud
.....................................................................................
			Spring and Kafka - Event Driven Microservices
.....................................................................................

Objective:
  Event Sourcing with Kafka..

Publish event into kafka broker...

Steps:

1.start kafka.

docker-compose  docker-compose-confl.yml up


2.Add kafka spring dependency
       <dependency>
            <groupId>org.springframework.kafka</groupId>
            <artifactId>spring-kafka</artifactId>
        </dependency>

3.KafkaTemplate
   Object is used to publish event into kafka Topic.

4.application.yml
spring:
  kafka:
    producer:
      bootstrap-servers: localhost:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer

  datasource:
    url: jdbc:h2:mem:testdb
    driverClassName: org.h2.Driver
    username: sa
    password:

  jpa:
    database-platform: org.hibernate.dialect.H2Dialect

  h2:
    console:
      enabled: true
      path: /h2


5.Coding:

package com.sunlife.eventsourcing;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.Random;
import java.util.UUID;
import java.util.concurrent.CompletableFuture;

@Service
public class EventService {

    @Autowired
    private KafkaTemplate<String, Object> template;

    public void addEvent(StockAddedEvent event) throws JsonProcessingException {
        EventRecord eventRecord = new EventRecord();
        eventRecord.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventRecord.setEventType(StockStatus.STOCK_ADDED.name());
        eventRecord.setEventId(UUID.randomUUID().getMostSignificantBits());
        eventRecord.setEntityId(event.getStockDetails().getName());
        eventRecord.setEventTime(LocalDateTime.now());
        CompletableFuture<SendResult<String, Object>> future = template.send("stock", eventRecord);
        future.whenComplete((result, ex) -> {
            if (ex == null) {
                System.out.println("Sent message=[" + eventRecord +
                        "] with offset=[" + result.getRecordMetadata().offset() + "]");
            } else {
                System.out.println("Unable to send message=[" +
                        eventRecord + "] due to : " + ex.getMessage());
            }
        });
    }

    public void addEvent(StockRemovedEvent event) throws JsonProcessingException {
        EventRecord eventRecord = new EventRecord();
        eventRecord.setEventData(new ObjectMapper().writeValueAsString(event.getStockDetails()));
        eventRecord.setEventType(StockStatus.STOCK_REMOVED.name());
        eventRecord.setEventId(UUID.randomUUID().getMostSignificantBits());
        eventRecord.setEntityId(event.getStockDetails().getName());
        eventRecord.setEventTime(LocalDateTime.now());
        CompletableFuture<SendResult<String, Object>> future = template.send("stock", eventRecord);
        future.whenComplete((result, ex) -> {
            if (ex == null) {
                System.out.println("Sent message=[" + eventRecord +
                        "] with offset=[" + result.getRecordMetadata().offset() + "]");
            } else {
                System.out.println("Unable to send message=[" +
                        eventRecord + "] due to : " + ex.getMessage());
            }
        });
    }


}
package com.sunlife.eventsourcing;

import lombok.Data;
import java.time.LocalDateTime;

@Data
public class EventRecord {
    private long eventId;
    private String eventType;
    private String entityId;
    private String eventData;
    private LocalDateTime eventTime;
}
package com.sunlife.eventsourcing;

import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaProducerConfig {

    @Bean
    public NewTopic createTopic() {
        return new NewTopic("stock", 3, (short) 1);
    }

    @Bean
    public Map<String, Object> producerConfig() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
                "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                JsonSerializer.class);
        return props;
    }

    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        return new DefaultKafkaProducerFactory<>(producerConfig());
    }

    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }

}
package com.sunlife.eventsourcing;

import jakarta.persistence.Entity;
import jakarta.persistence.GeneratedValue;
import jakarta.persistence.GenerationType;
import jakarta.persistence.Id;
import lombok.Data;
@Entity
@Data
public class Stock {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private long id;
    private String name;
    private int quantity;
    private String userName;

}
package com.sunlife.eventsourcing;

import lombok.Builder;
import lombok.Data;

@Data
@Builder
public class StockAddedEvent implements StockEvent {
    private  Stock stockDetails;
}
package com.sunlife.eventsourcing;


import com.fasterxml.jackson.core.JsonProcessingException;
import com.google.gson.Gson;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.*;

import java.time.LocalDate;
import java.time.LocalDateTime;
import java.util.List;

@RestController
public class StockController {
    @Autowired
    private EventService eventService;

    @Autowired
    private StockRepo repo;

    @PostMapping("/stock")
    public void addStock(@RequestBody Stock stockRequest) throws JsonProcessingException {
        StockAddedEvent event = StockAddedEvent.builder().stockDetails(stockRequest).build();

        List<Stock> existingStockList = repo.findByName(stockRequest.getName());

        if (existingStockList != null && existingStockList.size() > 0) {

            Stock existingStock = existingStockList.get(0);

            int newQuantity = existingStock.getQuantity() + stockRequest.getQuantity();

            existingStock.setQuantity(newQuantity);
            existingStock.setUserName(stockRequest.getUserName());
            repo.save(existingStock);

        } else {

            repo.save(stockRequest);
        }
        eventService.addEvent(event);
    }

    @DeleteMapping("/stock")
    public void removeStock(@RequestBody Stock stock) throws JsonProcessingException {
        StockRemovedEvent event = StockRemovedEvent.builder().stockDetails(stock).build();
        int newQuantity = 0;

        List<Stock> existingStockList = repo.findByName(stock.getName());

        if (existingStockList != null && existingStockList.size() > 0) {

            Stock existingStock = existingStockList.get(0);

            newQuantity = existingStock.getQuantity() - stock.getQuantity();

            if (newQuantity <= 0) {
                repo.delete(existingStock);
            } else {
                existingStock.setQuantity(newQuantity);
                existingStock.setUserName(stock.getUserName());
                repo.save(existingStock);
            }
        }
        eventService.addEvent(event);
    }

    @GetMapping("/stock")
    public List<Stock> getStock(@RequestParam("name") String name) throws JsonProcessingException {
        return repo.findByName(name);
    }


}
package com.sunlife.eventsourcing;

public interface StockEvent {
}
package com.sunlife.eventsourcing;

import lombok.Builder;
import lombok.Data;

@Builder
@Data
public class StockRemovedEvent implements StockEvent {
    private Stock stockDetails;
}

package com.sunlife.eventsourcing;

import java.util.List;

import org.springframework.data.repository.CrudRepository;

public interface StockRepo extends CrudRepository<Stock, Integer> {

    List<Stock> findByName(String name);
}
package com.sunlife.eventsourcing;

public enum StockStatus {
    STOCK_ADDED,
    STOCK_REMOVED
}
....................................................................................
			 Spring cloud Stream
....................................................................................

What is Spring Cloud Stream?

 Spring Cloud Stream is a Spring module that merges Spring Integration (which implements integration patterns) with Spring Boot.
The goal of this module is to allow the developer to focus solely on the business logic of event-driven applications, without worrying about the code to handle different types of message systems.

In fact, with Spring Cloud Stream, you can write code to produce/consume messages on Kafka, but the same code would also work if you used RabbitMQ, AWS Kinesis, AWS SQS, Azure EventHubs, etc!


Spring Cloud Stream is a framework for building highly scalable event-driven microservices connected with shared messaging systems.

The framework provides a flexible programming model built on already established and familiar Spring idioms and best practices, including support for persistent pub/sub semantics, consumer groups, and stateful partitions.

Spring Cloud Stream from Spring Cloud Function:

Spring Cloud Stream is based on Spring Cloud Function. Business logic can be written through simple functions.

The classic three interfaces of Java are used:

Supplier: a function that has output but no input; it is also called producer, publisher, source .
Consumer: a function that has input but no output, it is also called subscriber or sink.
Function: a function that has both input and output, is also called processor

		
				Spring Cloud Stream
					 |
 			    Kafka Google Pub sub RabbitMQ			
 


Binder Implementations:
 Binder is bridge api which connects Messaging providers.

RabbitMQ

Apache Kafka
Kafka Streams
Amazon Kinesis
Google PubSub (partner maintained)
Solace PubSub+ (partner maintained)
Azure Event Hubs (partner maintained)
Azure Service Bus (partner maintained)
AWS SQS (partner maintained)
AWS SNS (partner maintained)
Apache RocketMQ (partner maintained)

The core building blocks of Spring Cloud Stream are:

1.Destination Binders: Components responsible to provide integration with the external messaging systems.

Destination Bindings: Bridge between the external messaging systems and application code (producer/consumer) provided by the end user.

Message: The canonical data structure used by producers and consumers to communicate with Destination Binders (and thus other applications via external messaging systems).


Spring Cloud Stream application Types

1.Sources - java.util.function.Supplier
2.Sinks -java.util.function.Consumer
3.Processors -java.util.function.Function

Modern Spring Cloud Stream bindings works with functional Style rather than annotation style.


Two types of programming.

1.Publising events automatically
2.Publishing events manually.


1.Publising events automatically

=>Publisher 
=>Consumer
=>Processor

Note: 
 The publisher,consumers,processor are represented as "Functional Bean".

By default we dont need any configurations related to connecting kakfa, providing topic name....

package com.sunlife;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import java.util.UUID;
import java.util.function.Supplier;

@SpringBootApplication
public class SpringCloudStreamApp {

	public static void main(String[] args) {
		SpringApplication.run(SpringCloudStreamApp.class, args);
	}

	//producer which sends messages via functional
	//stringSupplier is function name , if you dont configure, then function name would be topic
	//name
	@Bean
	public Supplier<UUID> stringSupplier(){
		return ()->{
			 var uuid= UUID.randomUUID();
			 return uuid;
		};
	}

}

When you run this code, automatically the spring creates topic and starts publishing message into kafka - stream....


#Stream Configuration
spring:
  cloud:
    function:
      definition: stringSupplier;stringConsumer
    stream:
      bindings:
        stringSupplier-out-0:
          destination: randomUUid-topic
        stringConsumer-in-0:
          destination: randomUUid-topic
        stockEvent-out-0:
          destination: inventory-topic
#Bindiner(Kafka) Configuration
package com.sunlife;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.cloud.stream.function.StreamBridge;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
@RequestMapping("/api/publish")
public class StockController {

    @Autowired
    private StreamBridge streamBridge;

    @PostMapping
    public String publish(@RequestBody Stock stock){
            streamBridge.send("stockEvent-out-0",stock);
        return "Message Published";

    }
}
package com.sunlife;

public class Stock {
    private String id;
    private String status;

    public Stock(String id, String status) {
        this.id = id;
        this.status = status;
    }

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public String getStatus() {
        return status;
    }

    public void setStatus(String status) {
        this.status = status;
    }
}
.....................................................................................
				Data Management

Core Pattern:
1.Database Per Service Pattern
2.Shared Database

DataBase Per Service:
.....................

Context:
 You are building microservice app.
 Services need to persit data into some kind of databases
 For eg OrderService stores data into OrderDatabase , Customer Stores data into  Customer Database


Problem:
  What is the db arch in a microservice app?


Forces: (Issues you must address when you solve a problem)

=>Services must be lossly coupled so that they can be developed,deployed and scaled independently.

=>Some business transactions must enforce invariants that span multiple services. For example, the Place Order use case must verify that a new Order will not exceed the customer’s credit limit. Other business transactions, must "update data" owned by multiple services. - Update Operation across multiple services and multiple databases

=>some business transactions need to query data that is owned by multiple services. For example, the View Available Credit use must query the Customer to find the creditLimit and Orders to calculate the total amount of the open orders -Select Data across multiple services and multiple databases

=>Some queries must join data that is owned by multiple services. For example, finding customers in a particular region and their recent orders requires a join between customers and orders = Select data across multiple data bases and services

=>Databases must sometimes be replicated and sharded in order to scale


=>Different services have different data storage requirements. For some services, a relational database is the best choice. Other services might need a NoSQL database such as MongoDB, which is good at storing complex, unstructured data, or Neo4J, which is designed to efficiently store and query graph data

Solution:
=>Keep each microservice’s persistent data private to that service and accessible only via its API. 
=>A service’s transactions only involve its database (Local Transactions)

=>The service’s database is effectively part of the implementation of that service.  It cannot be accessed directly by other services.

=>Storage options:
  1.Private-tables-per-service – each service owns a set of tables that must only be accessed by that service
  2.Schema-per-service – each service has a database schema that’s private to that service
  3.Database-server-per-service – each service has it’s own database server.


Resulting context

Advantages:
 1.Helps ensure that the services are loosely coupled. Changes to one service’s    database does not impact any other services.

 2.Each service can use the type of database that is best suited to its needs. For       example, a service that does text searches could use ElasticSearch. A service that    manipulates a social graph could use Neo4j.

DisAdvantages:
 
 1.Implementing business transactions that span multiple services is not straightforward. 
 2.Distributed transactions are best avoided because of the CAP theorem.
 3.Moreover, many modern (NoSQL) databases don’t support them.
 4.Implementing queries that join data that is now in multiple databases is challenging.
 5.Complexity of managing multiple SQL and NoSQL databases


If you select "Data Per Service"

Each Service ===> Single Database - Recommended

Challanges
 =>Transactions Management - UPDATE,DELETE,INSERT
 =>Query Data =>Select,Joins

Solution:
 Transactions patterns

SAGA
 -2PC -Not Recommend
 -Choreography
 -Orchestration

Advanced Transaction:
 Transactional OutputBox

Query:
 CQRS Patterns
 API Compostion

           All DATA base Patterns built on the top of EventSourcing


SAGA: To manage database Transactions across multiple services...
.....

A service command typically needs to create/update/delete aggregates(rows) in the database and send messages/events to a message broker.

Saga works based on Event sourcing pattern.

For example, a service that participates in a saga needs to update business entities and send messages/events. Similarly, a service that publishes a domain event must update an aggregate and publish an event.



2PC:
  2 Phase Commit :
	Two-phase commit enables you to update multiple, disparate databases within a single transaction, and commit or roll back changes as a single unit-of-work.

SAGA implementation:
 
There are two design patterns :

1.Choreograph
2.Orchestration
 Both pattern is used to send and receive messages via brokers 
 Biz transactions are coordinated via message bus.


1.Choreograph:
  Choreography - each local transaction publishes domain events that trigger local transactions in other services

Flow:
1.The Order Service receives the POST /orders request and creates an Order in a PENDING state - in the local database
2.It then emits an Order Created event
3.The Customer Service’s event handler attempts to reserve credit
4.It then emits an event indicating the outcome
5.The OrderService’s event handler either approves or rejects the Order

Choregraphy pattern every service responsibility to send and listen messages.

Program:
=>H2 Database.
=>spring-Data-jpa
=>Spring-cloud-stream,spring-kafka,spring-cloud-stream-kafka-binder
=>Reactive Programming -WebFlux

java Reactive and Programming
1.Rxjava
2.Project Reactor..
3.SmallRye Mutiny

Three types of events
1.Data Event
2.Error Event
3.complete

Project uses two objects to represent producer...

1.Mono - He can publish only one event(data,error
2.Flux - He can publish 0...N events

Operators:
 apis to process the event stream - filtering,transaction,creation,aggreation...

What is webflux?
  It is spring wrapper for "Project reactor".

Why WebFlux?

Your web app is completly reactive 
Your web app runs in non blocking env - netty...

................
Project Structure:
 common-dto
 order-service
 inventory-service
 payment-service



The business workflow :

1.order-services receives a POST request for a new order
2.It places an order request in the DB in the ORDER_CREATED state and raises an event
3.payment-service listens to the event, confirms about the credit reservation
4.inventory-service also listens to the order-event and conforms the inventory reservation
5.order-service fulfills order or rejects the order based on the credit & inventory reservation status.

https://www.youtube.com/watch?v=ojDs2ep990A - Advanced Spring cloud stream.

common-dto
 -dto and event objects


















